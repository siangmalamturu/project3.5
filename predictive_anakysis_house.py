# -*- coding: utf-8 -*-
"""Predictive_Anakysis_House.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sd9zE6syzVmkat0PeWUXKQJOH573hKXD

### Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

!pip install xgboost

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.model_selection import RandomizedSearchCV

from sklearn.metrics import mean_absolute_error

"""### Data Understanding

Muat dataset
"""

df = pd.read_csv('dataset/train.csv')

"""Menampilkan 5 baris pertama data"""

print("Lima baris pertama dataset:")
print(df.head())

"""Menampilkan informasi umum dataset (jumlah baris, kolom, tipe data, non-null values)"""

print("\nInformasi umum dataset:")
df.info()

"""Menampilkan jumlah baris dan kolom"""

print(f"\nJumlah baris: {df.shape[0]}")
print(f"Jumlah kolom: {df.shape[1]}")

"""Mengecek missing values"""

print("\nJumlah missing values per kolom:")
print(df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False))

"""Menampilkan statistik deskriptif untuk kolom numerik"""

print("\nStatistik deskriptif untuk kolom numerik:")
print(df.describe())

"""### EDA dan Visualisasi Data

Distribusi SalePrice
"""

plt.figure(figsize=(10, 6))
sns.histplot(df['SalePrice'], kde=True, bins=50)
plt.title('Distribusi Harga Jual (SalePrice)')
plt.xlabel('Harga Jual ($)')
plt.ylabel('Frekuensi')
plt.show()

"""Transformasi log SalePrice untuk melihat distribusi yang lebih normal"""

plt.figure(figsize=(10, 6))
sns.histplot(np.log1p(df['SalePrice']), kde=True, bins=50)
plt.title('Distribusi Harga Jual (SalePrice) setelah Transformasi Log')
plt.xlabel('Log(Harga Jual)')
plt.ylabel('Frekuensi')
plt.show()

"""Korelasi antara fitur numerik dengan SalePrice"""

numeric_cols = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numeric_cols].corr()
top_correlated_features = correlation_matrix.nlargest(10, 'SalePrice')['SalePrice'].index

plt.figure(figsize=(12, 8))
sns.heatmap(df[top_correlated_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi Fitur Numerik Teratas dengan SalePrice')
plt.show()

"""Scatter plot beberapa fitur dengan SalePrice"""

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.scatterplot(x='GrLivArea', y='SalePrice', data=df)
plt.title('GrLivArea vs SalePrice')

plt.subplot(1, 3, 2)
sns.scatterplot(x='GarageCars', y='SalePrice', data=df)
plt.title('GarageCars vs SalePrice')

plt.subplot(1, 3, 3)
sns.scatterplot(x='OverallQual', y='SalePrice', data=df)
plt.title('OverallQual vs SalePrice')

plt.tight_layout()
plt.show()

"""Analisis fitur kategorikal (contoh: Neighborhood vs SalePrice)"""

plt.figure(figsize=(15, 7))
sns.boxplot(x='Neighborhood', y='SalePrice', data=df.sort_values('SalePrice', ascending=False))
plt.title('Harga Jual berdasarkan Neighborhood')
plt.xticks(rotation=90)
plt.show()

"""### Data Preparation

# Menangani Missing Values

Seperti yang terlihat di tahap Data Understanding, beberapa kolom memiliki missing values yang tinggi. Kita akan menentukan strategi imputasi atau penghapusan berdasarkan persentase missing values dan karakteristik fitur.

Menghitung persentase missing values
"""

missing_percentages = df.isnull().sum() / len(df) * 100
print("Persentase Missing Values per Kolom (lebih dari 0%):")
print(missing_percentages[missing_percentages > 0].sort_values(ascending=False))

"""Kolom dengan missing values yang sangat tinggi (>50%) mungkin lebih baik dihapus
Kolom: PoolQC, MiscFeature, Alley, Fence memiliki lebih dari 80% missing values
GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond memiliki sekitar 5% missing values
Bsmt* features (BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2) juga memiliki missing values.

Menentukan kolom numerik dan kategorikal
"""

numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df.select_dtypes(include='object').columns.tolist()

"""Karena 'Id' bukan fitur prediktif, kita akan menghapusnya dan 'SalePrice' adalah target, jadi kita pisahkan"""

numerical_cols.remove('Id')
numerical_cols.remove('SalePrice')

"""Menentukan fitur-fitur yang akan di-drop karena terlalu banyak missing values"""

features_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence']
df_cleaned = df.drop(columns=features_to_drop)

"""Perbarui daftar kolom numerik dan kategorikal setelah dropping"""

numerical_cols_cleaned = df_cleaned.select_dtypes(include=np.number).columns.tolist()
categorical_cols_cleaned = df_cleaned.select_dtypes(include='object').columns.tolist()

"""Hapus 'SalePrice' dari numerical_cols_cleaned karena itu target"""

numerical_cols_cleaned.remove('SalePrice')

"""# Strategi Imputasi Missing Values

Untuk kolom numerik, kita bisa menggunakan imputasi median atau mean. Untuk kolom kategorikal, kita bisa menggunakan imputasi mode atau menandai 'None' jika missing value memang berarti tidak ada fitur tersebut (misalnya, tidak ada basement, tidak ada garasi).

Mari kita cek ulang missing values setelah drop beberapa kolom
"""

print("\nMissing values setelah drop kolom:")
print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0].sort_values(ascending=False))

"""Kolom kategorikal yang missing values-nya berarti 'None' (tidak ada):"""

cat_none_cols = ['MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
                 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'FireplaceQu']
for col in cat_none_cols:
    if col in df_cleaned.columns:
        df_cleaned[col] = df_cleaned[col].fillna('None')

"""Kolom numerik yang missing, isi dengan median"""

numerical_cols_for_imputation = ['LotFrontage']
for col in numerical_cols_for_imputation:
    if col in df_cleaned.columns and df_cleaned[col].isnull().any():
        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())

"""Untuk MasVnrArea dan GarageYrBlt, kita isi 0 karena sesuai dengan 'None' pada fitur kategorinya"""

if 'MasVnrArea' in df_cleaned.columns:
    df_cleaned['MasVnrArea'] = df_cleaned['MasVnrArea'].fillna(0)
if 'GarageYrBlt' in df_cleaned.columns:
    df_cleaned['GarageYrBlt'] = df_cleaned['GarageYrBlt'].fillna(0)

print("\nMissing values setelah imputasi:")
print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0].sort_values(ascending=False))

"""# 3. Feature Engineering (Sederhana)
Membuat beberapa fitur baru yang mungkin relevan
"""

df_cleaned['TotalSF'] = df_cleaned['1stFlrSF'] + df_cleaned['2ndFlrSF'] + df_cleaned['TotalBsmtSF']
df_cleaned['Age'] = df_cleaned['YrSold'] - df_cleaned['YearBuilt']
df_cleaned['RemodAge'] = df_cleaned['YrSold'] - df_cleaned['YearRemodAdd']
df_cleaned.loc[df_cleaned['Age'] < 0, 'Age'] = 0
df_cleaned.loc[df_cleaned['RemodAge'] < 0, 'RemodAge'] = 0

"""Fitur yang bisa di-drop setelah feature engineering atau karena redundansi"""

features_to_drop_after_fe = ['YearBuilt', 'YearRemodAdd', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF']
df_cleaned = df_cleaned.drop(columns=features_to_drop_after_fe, errors='ignore')

"""Log Transform pada SalePrice (Target Variable)"""

df_cleaned['SalePrice_Log'] = np.log1p(df_cleaned['SalePrice'])

"""# 4. Encoding Fitur Kategorikal

Kita akan menggunakan One-Hot Encoding untuk fitur kategorikal. Ini mengubah kolom kategorikal menjadi representasi numerik biner.

Perbarui daftar kolom kategorikal setelah feature engineering dan dropping
"""

categorical_cols_final = df_cleaned.select_dtypes(include='object').columns.tolist()

"""Pisahkan fitur dan target"""

X = df_cleaned.drop(columns=['SalePrice', 'SalePrice_Log']) # Drop SalePrice original dan Log target untuk fitur
y = df_cleaned['SalePrice_Log']

"""Pisahkan kolom numerik dan kategorikal dari X"""

numerical_features_final = X.select_dtypes(include=np.number).columns.tolist()
categorical_features_final = X.select_dtypes(include='object').columns.tolist()

"""Pipeline untuk fitur numerik"""

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

"""Pipeline untuk fitur kategorikal"""

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')), # Untuk jaga-jaga kalau ada lagi
    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' untuk kolom baru di test set
])

"""Gabungkan pipeline menggunakan ColumnTransformer"""

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features_final),
        ('cat', categorical_transformer, categorical_features_final)
    ])

"""Terapkan preprocessing ke data (ini hanya untuk melihat bentuk akhir, akan diterapkan di pipeline modeling)"""

X_preprocessed = preprocessor.fit_transform(X)

print("\nBentuk X setelah preprocessing:", X_preprocessed.shape)
print("Jumlah fitur numerik akhir:", len(numerical_features_final))
print("Jumlah fitur kategorikal setelah One-Hot Encoding:", X_preprocessed.shape[1] - len(numerical_features_final))

"""### Modelling"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Bentuk X_train: {X_train.shape}")
print(f"Bentuk X_test: {X_test.shape}")
print(f"Bentuk y_train: {y_train.shape}")
print(f"Bentuk y_test: {y_test.shape}")

"""Inisialisasi Model"""

models = {
    'Linear Regression': Pipeline(steps=[('preprocessor', preprocessor),
                                         ('regressor', LinearRegression())]),
    'Random Forest Regressor': Pipeline(steps=[('preprocessor', preprocessor),
                                                ('regressor', RandomForestRegressor(random_state=42))]),
    'XGBoost Regressor': Pipeline(steps=[('preprocessor', preprocessor),
                                          ('regressor', XGBRegressor(random_state=42, n_jobs=-1))])
}

"""Melatih dan Mengevaluasi Model Dasar (Baseline Models)"""

print("\n--- Melatih dan Mengevaluasi Model Dasar ---")
results = {}
for name, model in models.items():
    print(f"\nMelatih {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluasi menggunakan MSE dan R2 Score pada data yang sudah di-log
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse) # RMSE lebih mudah diinterpretasi karena skalanya sama dengan target
    r2 = r2_score(y_test, y_pred)

    results[name] = {'RMSE': rmse, 'R2_Score': r2}

    print(f"{name} - RMSE: {rmse:.4f}")
    print(f"{name} - R2 Score: {r2:.4f}")

    # Visualisasi Prediksi vs Aktual (untuk salah satu model, misal XGBoost)
    if name == 'XGBoost Regressor':
        plt.figure(figsize=(10, 6))
        sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.3})
        plt.title(f'Prediksi vs Aktual (Log Transformed) - {name}')
        plt.xlabel('Harga Jual Aktual (Log)')
        plt.ylabel('Harga Jual Prediksi (Log)')
        plt.grid(True)
        plt.show()

"""Menampilkan hasil perbandingan model dasar"""

print("\n--- Hasil Perbandingan Model Dasar ---")
results_df = pd.DataFrame(results).T
print(results_df.sort_values(by='RMSE'))

"""Hyperparameter Tuning untuk Model Terbaik (XGBoost)"""

print("\n--- Hyperparameter Tuning (XGBoost Regressor) dengan RandomizedSearchCV ---")

# Parameter distribution yang lebih luas karena RandomizedSearchCV akan mengambil sampel secara acak
param_dist_xgb = {
    'regressor__n_estimators': [100, 200, 300, 400, 500, 600],
    'regressor__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'regressor__max_depth': [3, 4, 5, 6, 7, 8],
    'regressor__subsample': [0.7, 0.8, 0.9, 1.0], # Tambahan parameter untuk eksplorasi
    'regressor__colsample_bytree': [0.7, 0.8, 0.9, 1.0] # Tambahan parameter
}

# Buat model XGBoost dalam pipeline
xgb_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                     ('regressor', XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')) # Tambahkan tree_method='hist'
                                    ])

# Inisialisasi RandomizedSearchCV
random_search_xgb = RandomizedSearchCV(estimator=xgb_model_pipeline,
                                       param_distributions=param_dist_xgb,
                                       n_iter=20, # Jumlah kombinasi parameter yang akan dicoba (sesuaikan ini)
                                       cv=5, # 5-fold cross-validation
                                       scoring='neg_root_mean_squared_error', # Skoring untuk RMSE
                                       n_jobs=-1, # Gunakan semua core CPU
                                       random_state=42, # Penting untuk reproduktifitas
                                       verbose=1)

print(f"Memulai RandomizedSearchCV untuk XGBoost Regressor (mencoba {random_search_xgb.n_iter} kombinasi)...")
random_search_xgb.fit(X_train, y_train)

print(f"\nParameter terbaik untuk XGBoost: {random_search_xgb.best_params_}")
print(f"RMSE terbaik dari Cross-Validation: {-random_search_xgb.best_score_:.4f}")

"""Evaluasi model XGBoost terbaik pada test set"""

best_xgb_model = random_search_xgb.best_estimator_
y_pred_best_xgb = best_xgb_model.predict(X_test)

mse_best_xgb = mean_squared_error(y_test, y_pred_best_xgb)
rmse_best_xgb = np.sqrt(mse_best_xgb)
r2_best_xgb = r2_score(y_test, y_pred_best_xgb)

print(f"\n--- Evaluasi XGBoost Terbaik (setelah tuning) ---")
print(f"RMSE (Test Set): {rmse_best_xgb:.4f}")
print(f"R2 Score (Test Set): {r2_best_xgb:.4f}")

"""Visualisasi Prediksi vs Aktual untuk model terbaik setelah tuning"""

plt.figure(figsize=(10, 6))
sns.regplot(x=y_test, y=y_pred_best_xgb, scatter_kws={'alpha':0.3})
plt.title(f'Prediksi vs Aktual (Log Transformed) - XGBoost (Tuned)')
plt.xlabel('Harga Jual Aktual (Log)')
plt.ylabel('Harga Jual Prediksi (Log)')
plt.grid(True)
plt.show()

"""### Evaluation

Menghitung Metrik Evaluasi pada Skala Log (sesuai target model)
"""

rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_best_xgb))
mae_log = mean_absolute_error(y_test, y_pred_best_xgb)
r2_log = r2_score(y_test, y_pred_best_xgb)

print("--- Metrik Evaluasi Model Terbaik (pada skala Log) ---")
print(f"RMSE (Log Scale): {rmse_log:.4f}")
print(f"MAE (Log Scale): {mae_log:.4f}")
print(f"R2 Score (Log Scale): {r2_log:.4f}")

"""Mengubah Prediksi Kembali ke Skala Asli untuk Interpretasi yang Lebih Baik"""

y_test_original = np.expm1(y_test)
y_pred_original = np.expm1(y_pred_best_xgb)

"""Menghitung Metrik Evaluasi pada Skala Asli"""

rmse_original = np.sqrt(mean_squared_error(y_test_original, y_pred_original))
mae_original = mean_absolute_error(y_test_original, y_pred_original)
r2_original = r2_score(y_test_original, y_pred_original)

print("\n--- Metrik Evaluasi Model Terbaik (pada skala Harga Asli) ---")
print(f"RMSE (Original Scale): ${rmse_original:,.2f}")
print(f"MAE (Original Scale): ${mae_original:,.2f}")
print(f"R2 Score (Original Scale): {r2_original:.4f}")

"""Visualisasi Hasil Prediksi vs Aktual pada Skala Asli"""

plt.figure(figsize=(12, 7))
sns.scatterplot(x=y_test_original, y=y_pred_original, alpha=0.6)
plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.title('Harga Aktual vs Harga Prediksi (Skala Asli)')
plt.xlabel('Harga Aktual Properti ($)')
plt.ylabel('Harga Prediksi Properti ($)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Residual Plot

Melihat distribusi error (residual = aktual - prediksi)
"""

residuals = y_test_original - y_pred_original
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, bins=50)
plt.title('Distribusi Residual (Error Prediksi)')
plt.xlabel('Residual (Harga Aktual - Harga Prediksi) ($)')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred_original, y=residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot (Prediksi vs Residual)')
plt.xlabel('Harga Prediksi Properti ($)')
plt.ylabel('Residual ($)')
plt.grid(True)
plt.show()





